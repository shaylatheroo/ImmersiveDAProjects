{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: 12 Princesses\n",
    "\n",
    "From this link (https://notebooks.azure.com/priesterkc/projects/DABmaterial/tree/Lv2%20Data%20Analytics/datasets), download the 12dancingprincesses.txt file. Then read the file and use the NLTK library to tokenize each word in the text. Below is some code to help you get started with loading the file. The text in square brackets [] is where you would add your own code.\n",
    "\n",
    "[create an empty list here to hold the tokens at the end]\n",
    "\n",
    "with open([the name of the file], 'r') as f:\n",
    "    for line in f:\n",
    "        cline = line.strip() #get rid of newline character\n",
    "\n",
    "    if cline == '': pass  #this will skip over lines that only had a newline and are now blank\n",
    "    else:\n",
    "        tknls = [write the function to tokenize the words]\n",
    "\n",
    "    for token in tknls:\n",
    "        [write the function to append each token to the empty list you created at the start of this code]\n",
    "After using that code to load and tokenizing each word, then remove the punctuation and filler words (stopwords) from the list of tokens. Lastly, get the top 10 words from the text.\n",
    "\n",
    "Upload your Jupyter notebook to Github and submit the URL for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#this is sample data\n",
    "from nltk.corpus import names  \n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknls = []\n",
    "nojunkls = []\n",
    "\n",
    "with open('12dancingprincesses.txt', 'r', encoding = \"windows-1252\") as f:\n",
    "    for line in f:\n",
    "        line = line.lower()\n",
    "        cline = line.strip()\n",
    "        \n",
    "        if cline == '': pass  #this will skip over lines that only had a newline and are now blank\n",
    "        else:\n",
    "            tknls = tknls + word_tokenize(cline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a weird one with trying to sort out the encoding on the file - thanks to Atom and their auto-detect, we're okay and everything has read in properly! Now to kick out the junk we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = punctuation + \"‘’\"\n",
    "new_token = []\n",
    "\n",
    "for token in tknls:\n",
    "    if token not in punctuation:\n",
    "        new_token.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords\n",
    "\n",
    "rm_count = 0\n",
    "new_words = []  #list to hold new words\n",
    "\n",
    "for token in new_token:\n",
    "    if token not in eng_stopwords:\n",
    "        new_words.append(token)\n",
    "    else: rm_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soldier', 19),\n",
       " ('princesses', 17),\n",
       " ('said', 16),\n",
       " ('king', 15),\n",
       " ('twelve', 11),\n",
       " ('went', 11),\n",
       " ('came', 10),\n",
       " ('eldest', 10),\n",
       " ('one', 7),\n",
       " ('night', 7)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_nw = FreqDist(new_words)\n",
    "fd_nw.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
